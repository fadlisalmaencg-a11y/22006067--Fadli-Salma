# Salma Fadli G2 Finance
# Assurance SantÃ©
![photo de salma fadli.jpeg](https://github.com/fadlisalmaencg-a11y/DS-2025/blob/main/photo%20de%20salma%20fadli.jpeg?raw=true)
# ğŸ“Š Compte Rendu dâ€™Analyse & Clustering du Dataset *Insurance Charges*

## ğŸ“ 1. Introduction
Ce rapport prÃ©sente une analyse exploratoire et une application de techniques de **clustering** sur le dataset *Insurance Charges*.  
Lâ€™objectif est de :

- comprendre la structure des donnÃ©es,  
- prÃ©parer un prÃ©-traitement adaptÃ©,  
- appliquer diffÃ©rents algorithmes de clustering,  
- comparer les performances Ã  lâ€™aide dâ€™indicateurs (ex : silhouette),  
- visualiser les rÃ©sultats via PCA.  

Le dataset est chargÃ© directement depuis une source publique, sans besoin de fichier ZIP.

---

## ğŸ“‚ 2. Chargement du Dataset
Le dataset provient de :


Il contient les colonnes suivantes :

- **age** : Ã¢ge de lâ€™assurÃ©  
- **sex** : homme / femme  
- **bmi** : indice de masse corporelle  
- **children** : nombre dâ€™enfants  
- **smoker** : fumeur ou non  
- **region** : zone gÃ©ographique  
- **charges** : montant annuel des frais mÃ©dicaux  

---

## ğŸ” 3. Analyse exploratoire (EDA)

### âœ” AperÃ§u gÃ©nÃ©ral
- Aucune valeur manquante.  
- Colonnes numÃ©riques : `age`, `bmi`, `children`, `charges`  
- Colonnes catÃ©gorielles : `sex`, `smoker`, `region`

### âœ” Observations principales
- Les fumeurs ont des charges beaucoup plus Ã©levÃ©es.  
- Le BMI influence fortement les dÃ©penses mÃ©dicales.  
- Des groupes naturels semblent exister (fumeurs/non-fumeurs avec BMI Ã©levÃ©).  

---

## âš™ï¸ 4. PrÃ©-traitement

### Pipeline :
- **StandardScaler** pour les variables numÃ©riques  
- **OneHotEncoding** pour les variables catÃ©gorielles  
- Construction dâ€™une matrice prÃªte pour le clustering  
- **PCA (2 composantes)** pour visualisation simplifiÃ©e  

---

## ğŸ¤– 5. MÃ©thodes de Clustering AppliquÃ©es

### ### â­ 5.1 K-Means
- Test des valeurs de k entre 2 et 8  
- Analyse via :
  - **mÃ©thode du coude (inertia)**
  - **score silhouette**

âœ” **k optimal** â‰ˆ 3  
âœ” **Silhouette** â‰ˆ 0.40 (indicatif)  

### ### â­ 5.2 Agglomerative Clustering
- Utilisation du mÃªme k optimal  
- Score silhouette lÃ©gÃ¨rement infÃ©rieur Ã  K-Means  

### ### â­ 5.3 DBSCAN
- Permet de dÃ©tecter :
  - clusters de forme irrÃ©guliÃ¨re  
  - points bruit  
- RÃ©sultats sensibles au choix de `eps`  

---

## ğŸ“‰ 6. Visualisations

### PCA 2D avec clusters :
- Les clusters sont bien sÃ©parÃ©s surtout selon :
  - statut fumeur  
  - BMI Ã©levÃ©  
  - charges mÃ©dicales importantes  

Les zones sont visuellement cohÃ©rentes :  
fumeurs Ã  BMI Ã©levÃ© forment un cluster trÃ¨s distinct.

---

## ğŸ 7. Conclusion

- **K-Means (k = 3)** offre la meilleure segmentation globale.  
- Les clusters identifiÃ©s correspondent Ã  des profils clairs :
  1. Fumeurs â†’ charges trÃ¨s Ã©levÃ©es  
  2. Non-fumeurs, BMI modÃ©rÃ©  
  3. Jeunes assurÃ©s, charges faibles  

- **Agglomerative** : rÃ©sultats acceptables mais moins performants.  
- **DBSCAN** : utile pour dÃ©tecter anomalies / bruit.

---

## ğŸ’» 8. Code Python utilisÃ© (Google Colab)

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score

import warnings
warnings.filterwarnings("ignore")

# Charger dataset sans ZIP
url = "https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv"
df = pd.read_csv(url)
df.head()

# Colonnes
num_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()
cat_cols = df.select_dtypes(include=['object']).columns.tolist()

# PrÃ©processing
numeric = Pipeline([('scaler', StandardScaler())])
categorical = Pipeline([('ohe', OneHotEncoder(sparse=False))])

preprocess = ColumnTransformer([
    ('num', numeric, num_cols),
    ('cat', categorical, cat_cols)
])

X = preprocess.fit_transform(df)

# PCA
pca = PCA(2)
X_pca = pca.fit_transform(X)

# K-Means
scores = []
for k in range(2,8):
    km = KMeans(n_clusters=k, n_init=10, random_state=42)
    labels = km.fit_predict(X)
    scores.append(silhouette_score(X, labels))

best_k = 2 + np.argmax(scores)
kmeans = KMeans(n_clusters=best_k, n_init=20, random_state=42)
labels_k = kmeans.fit_predict(X)

# Visualisation
plt.figure(figsize=(8,5))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=labels_k, palette="tab10")
plt.title(f"Clustering K-Means (k={best_k}) â€” PCA")
plt.show()



